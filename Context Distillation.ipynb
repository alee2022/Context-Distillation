{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d049d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "import pandas as pd\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/deep/u/andleerew/final_proj/cache'\n",
    "tf_cache = '/deep/u/andleerew/final_proj/cache'\n",
    "\n",
    "API_KEY = 'AIzaSyAoIA0ZUJ8Q6fEpqJjgkHeiOe58zQUB0NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0e4b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Datasets - Real Toxicity Prompts\n",
    "prompts_df = pd.read_json(\"/deep/u/andleerew/final_proj/real-toxicity-prompts/realtoxicityprompts-data/prompts.jsonl\", lines=True)\n",
    "prompts_shuffled = prompts_df.sample(frac=1)\n",
    "test_prompts = prompts_shuffled[prompts_shuffled[\"challenging\"]==True]\n",
    "test_prompts = [prompt[\"text\"] for prompt in test_prompts[\"prompt\"]]\n",
    "train_prompts = prompts_shuffled[prompts_shuffled[\"challenging\"]==False]\n",
    "train_prompts = [prompt[\"text\"] for prompt in train_prompts[\"prompt\"]]\n",
    "teacher_context = \"Do not use any toxic, insulting, profane, or sexually explicit language while completing the following prompt: \"\n",
    "student_context = \"Complete the following prompt: \"\n",
    "empty_context = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d28ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up Labels for Distillation\n",
    "teacher_train_response = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/t5-v1_1-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-v1_1-base\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ec3d9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73517b3f2f9b4997af56f5f03f27b0c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_SIZE = 1024\n",
    "BATCH_SIZE = 16\n",
    "tr_prompts = train_prompts[:TRAIN_SIZE]\n",
    "TRAIN_BATCH = int(TRAIN_SIZE / BATCH_SIZE)\n",
    "progress = tqdm(range(TRAIN_BATCH))\n",
    "for i in range(TRAIN_BATCH):\n",
    "    input_text = tr_prompts[BATCH_SIZE * i:BATCH_SIZE * (i+1)]\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids)\n",
    "    teacher_train_response += tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f64b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, model_name, context_token, truncation=512):\n",
    "        self.__tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.__truncation = truncation\n",
    "        self.__context = context_token\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        return self.__tokenizer(\n",
    "            self.__context + sentence,\n",
    "            truncation=True,\n",
    "            max_length=self.__truncation,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "    def decode(self, word_ids, *args, **kwargs):\n",
    "        return self.__tokenizer.decode(word_ids, *args, **kwargs)\n",
    "    \n",
    "    def __call__(self, sentence):\n",
    "        return self.encode(sentence)\n",
    "    \n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        return self.__tokenizer.pad_token_id\n",
    "    \n",
    "teacher_tokenizer = Tokenizer('t5-base', teacher_context)\n",
    "student_tokenizer = Tokenizer('t5-base', student_context)\n",
    "response_tokenizer = Tokenizer('t5-base', empty_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e200eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1024\n",
      "Number of test examples: 1024\n"
     ]
    }
   ],
   "source": [
    "class PromptDataset:\n",
    "    def __init__(self, data, label, pr_tokenizer, re_tokenizer):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.prompt_tokenizer = pr_tokenizer\n",
    "        self.response_tokenizer = re_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        tokenized_input = self.prompt_tokenizer(sentence)\n",
    "        tokenized_label = self.response_tokenizer(label)\n",
    "        \n",
    "        return (\n",
    "            tokenized_input['input_ids'].squeeze(),\n",
    "            tokenized_input['attention_mask'].squeeze(),\n",
    "            tokenized_label['input_ids'].squeeze(),\n",
    "            tokenized_label['attention_mask'].squeeze()\n",
    "        )\n",
    "    \n",
    "teacher_train_ds = PromptDataset(train_prompts[:TRAIN_SIZE], teacher_train_response, teacher_tokenizer, response_tokenizer)\n",
    "student_train_ds = PromptDataset(train_prompts[:TRAIN_SIZE], teacher_train_response, student_tokenizer, response_tokenizer)\n",
    "\n",
    "\n",
    "print('Number of training examples:', len(teacher_train_ds))\n",
    "print('Number of test examples:', len(student_train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26fde420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_dl = DataLoader(student_train_ds, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4df2a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8450, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Definint Loss for Context Distillation\n",
    "class DistillationLoss:\n",
    "    def __init__(self, temp=1):\n",
    "        self.temp = temp\n",
    "    \n",
    "    def __call__(self, teacher_logits, student_logits):\n",
    "        t = self.temp\n",
    "        \n",
    "        loss = -(\n",
    "            (teacher_logits / t).softmax(dim=-1) * (student_logits / t).log_softmax(dim=-1)\n",
    "        ).sum(dim=-1).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "def test_distillation_loss():\n",
    "    t_input_ids, t_attention_mask, t_labels, t_dam = teacher_train_ds[0]\n",
    "    s_input_ids, s_attention_mask, s_labels, s_dam = student_train_ds[0]\n",
    "\n",
    "    output_teacher = T5ForConditionalGeneration.from_pretrained('t5-base')(\n",
    "        input_ids=t_input_ids.unsqueeze(dim=0), \n",
    "        attention_mask=t_attention_mask.unsqueeze(dim=0),\n",
    "        labels = t_labels.unsqueeze(dim=0),\n",
    "        decoder_attention_mask = t_dam.unsqueeze(dim=0)\n",
    "    )\n",
    "\n",
    "    output_student = T5ForConditionalGeneration.from_pretrained('t5-base')(\n",
    "        input_ids=s_input_ids.unsqueeze(dim=0), \n",
    "        attention_mask=s_attention_mask.unsqueeze(dim=0),\n",
    "        labels = s_labels.unsqueeze(dim=0),\n",
    "        decoder_attention_mask = s_dam.unsqueeze(dim=0)\n",
    "    )\n",
    "    \n",
    "    criterion = DistillationLoss(temp=1)\n",
    "    loss = criterion(output_teacher.logits, output_student.logits)\n",
    "    \n",
    "    print(loss)\n",
    "    \n",
    "test_distillation_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18bc3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "class LightningT5(pl.LightningModule):\n",
    "    def __init__(self, model_name, tokenizer):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.load_model(model_name)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def load_model(self, model_name):\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.t5.generate(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.t5.parameters(), lr=1e-5)\n",
    "    \n",
    "    def _step(self, batch):\n",
    "        input_ids, attention_mask, labels, decoder_attention_mask = batch\n",
    "        labels[labels[:] == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return self.t5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        ).loss\n",
    "    \n",
    "    def training_step(self, batch, idx):\n",
    "        loss = self._step(batch)\n",
    "        return loss\n",
    "    \n",
    "    def save_pretrained(self, path):\n",
    "        self.t5.save_pretrained(path)\n",
    "    \n",
    "    validation_step = training_step\n",
    "    test_step = training_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1590f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningDistilledT5(LightningT5):\n",
    "    def __init__(self, student_model_name, tokenizer, teacher_model, freeze_teacher=True, alpha=0, temp=1):\n",
    "        super().__init__(student_model_name, tokenizer)\n",
    "        self.teacher_model = teacher_model\n",
    "        \n",
    "        if freeze_teacher:\n",
    "            for p in self.teacher_model.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "        self.criterion = DistillationLoss(temp=temp, alpha=alpha)  \n",
    "    \n",
    "    def _step(self, batch):\n",
    "        input_ids, attention_mask, labels, decoder_attention_mask = batch\n",
    "        labels[labels[:] == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        student_output = self.t5(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=self.t5._shift_right(labels),\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "        \n",
    "        teacher_output = self.teacher_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=self.t5._shift_right(labels),\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "        \n",
    "        student_logits = student_output.logits\n",
    "        teacher_logits = teacher_output.logits\n",
    "        \n",
    "        return self.criterion(teacher_logits, student_logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c970ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, max_epochs, filename):\n",
    "        self.model = model\n",
    "        self.max_epochs = max_epochs\n",
    "        self.filename = filename\n",
    "        \n",
    "    def train(self, train_dl, max_steps=None):\n",
    "        gc.collect()\n",
    "        \n",
    "        model = self.model\n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            max_epochs=self.max_epochs\n",
    "        )\n",
    "        \n",
    "        trainer.fit(model, train_dl)\n",
    "        model.save_pretrained(self.filename)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_or_load_pretrained(self, train_dl, force=False, **kwargs):\n",
    "        if os.path.exists(self.filename) and not force:\n",
    "            self.model.load_model(self.filename)\n",
    "            return self.model\n",
    "        \n",
    "        return self.train(train_dl, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b027a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e658e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_t5_base_1_0 = Trainer(\n",
    "    LightningDistilledT5(\n",
    "        't5-base', \n",
    "        Tokenizer('t5-base', student_context), \n",
    "        finetuned_t5_base.t5,\n",
    "        temp=1,\n",
    "        alpha=0.5\n",
    "    ), \n",
    "    2, \n",
    "    'distillation_1_0'\n",
    ").train_or_load_pretrained(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd23835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
